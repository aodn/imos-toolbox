#summary Lists the pre-processing routines that are currently available.
#labels Support,Featured

<wiki:toc/>

= Pre-processing routines =

The following routines are available in the Toolbox since version 2.4 :

== Data time start redefinition - timeStartPP == 

timeStartPP allows modification of a data set's starting time.

This routine prompts the user to enter a new starting time for the data set. All time values are then offset accordingly. Useful for  data sets which were retrieved from an instrument with an unreliable or unset clock knowing the actual switch on date.

== Data time offset - timeOffsetPP == 

timeOffsetPP prompts the user to provide a time offset value (in hours) in order to apply time correction from local time to UTC time to each of the given data sets.

All IMOS datasets should be provided in UTC time. Collected data may not necessarily have been captured in UTC time, so a correction must be made before the data can be considered to be in an IMOS compatible format. 

The local time zone information associated to the data set is retrieved from the field !TimeZone in the table !DeploymentData from the deployment database and then converted into an offset in hours via the configuration file timeOffsetPP.txt. See configuration file example below :

{{{
% This file specifies default time offset values as used by the timeOffsetPP
% function. Each line is of the format:
%
%   timezone, offset
%
% where timezone is the time zone code, and offset is the value to be used to
% convert from that time zone to UTC/GMT, in hours.

UTC,      0.0
GMT,      0.0
UTC/GMT,  0.0

WST,     -8.0
AWST,    -8.0
WDT,     -9.0
AWDT,    -9.0

CST,     -9.5
ACST,    -9.5
CDT,     -10.5
ACDT,    -10.5

EST,     -10.0
AEST,    -10.0
EDT,     -11.0
AEDT,    -11.0

NZST,    -12.0
NZDT,    -13.0
}}}

timeOffsetPP also supports any timezone with the following notation : UTC+4, UTC-10, etc...

In addition, do not forget to document the global attribute local_time_zone (= +10 for EST timezone for example) so that users easily know which offset to apply to be back in local time. Actually if your data is in local time and consistent with the local time where the instrument is deployed, then you can even use the deployment database field !TimeZone in the table !DeploymentData to automatically document this attribute in your global_atributes.txt configuration file :

{{{
N, local_time_zone     		= [mat (-1)*str2double(readProperty('[ddb TimeZone]', ['Preprocessing' filesep 'timeOffsetPP.txt'], ','))]
}}}


== Metadata time offset - timeMetaOffsetPP == 

timeMetaOffsetPP prompts the user to provide a time offset value (in hours) in order to apply time correction from local time to UTC time to each of the metadata information retrieved for given data sets.

As for time data, all IMOS time metadata should be provided in UTC time. Metadata in deployment database may not necessarily have been documented in UTC time, so a correction must be made before the metadata can be considered to be in an IMOS compatible format.

Again, the local time zone information associated to the metadata is retrieved from the field !TimeZone in the table !DeploymentData from the deployment database and then converted into an offset in hours via the configuration file timeOffsetPP.txt. See configuration file example [#Data_time_offset_-_timeOffsetPP above].

Eventually, do not forget to document the global attribute local_time_zone so that users easily know which offset to apply to be back in local time.

== Variable scale/offset - variableOffsetPP == 

variableOffsetPP allows the user to apply a linear offset and scale to a variable in the given data sets. The output data in the NetCDF file for this variable will be different from the input data present in the original file.

It displays a dialog which allows the user to apply linear offsets and scales to any variable in the given data sets. The variable data is modified as follows:

{{{
   data = offset + (scale * data)
}}}

A variableOffsetPP.txt configuration file allows the user to define default scale/offset values for identified variables. See example below :

{{{
% Default scale/offset values used by variableOffsetPP for different variable 
% types. Each entry is of the form:
%
%   variable_name = offset_value, scale_value
%
% If a variable is not listed in this file, offset/scale values of 0.0/1.0 
% are used.
%

CNDC = 0, 0.01
}}}

== Relative pressure calculation - pressureRelPP == 

pressureRelPP adds a PRES_REL variable to the given data sets, if not already exist and if they contain a PRES variable.

PRES values are taken and an offset is applied to create PRES_REL values. Useful for instruments which pressure sensor is providing absolute pressure information, so that we can also have a relative pressure (usually to the surface) to be consistent with Seabird pressure sensor for example as they usually provide relative pressure.

A pressureRelPP.txt configuration file allows the user to define default offset values to be applied for identified instrument. See example below :

{{{
% Default pressure offset value used by pressureRelPP adding it to PRES variable 
% to create PRES_REL variable.
%
% In order to be able to apply different offsets for different sensors, 
% the considered offsets is the one in front of the matching 'source' 
% global atribute value. If no relevant 'source', use default.
%
% Each line is of the format:
%
%   source, offset
%

default,									-10.1325
RBR,										-10.2
}}}

Here, the default value -10.1325 is the actual value applied by Seabird on all their pressure sensor before outputting data in ASCII file format (.cnv).

== Depth calculation - depthPP == 

depthPP adds a DEPTH variable to the given data sets, if not already exist. Useful as during strong current event, an instrument on a mooring is not likely to stay at its nominal depth but may record data at lower depth as the mooring is laying down.

http://imos-toolbox.googlecode.com/svn/wiki/images/Mooring_knocked_down3.png

This function uses the [http://www.teos-10.org/ Gibbs-SeaWater toolbox (TEOS-10)] to derive depth data from pressure and latitude.

When a data set is loaded without using a deployment database, DEPTH will be calculated from PRES or PRES_REL variables present in the dataset. Without any latitude metadata information, 1 dbar ~= 1 m is assumed. If latitude metadata information is documented then DEPTH will be updated taking this new information into account using the !Gibbs-SeaWater toolbox.

When a data set is loaded using a deployment database, DEPTH will be calculated from PRES or PRES_REL variables present in the current instrument dataset or in the ones from the nearest pressure sensor on the mooring (1 or 2 nearest pressure sensors are used when possible). Latitude metadata information si taken into account using the !Gibbs-SeaWater toolbox. If latitude metadata information is updated then DEPTH will be updated taking this new information into account.

When an instrument data set doesn't contain any pressure/depth information, then the nearest pressure sensor (1 or 2 when possible) data on the mooring is considered and depth for the current instrument is linearly interpolated knowing the geometry/design of the mooring. Indeed, either global attributes instrument_nominal_height or instrument_nominal_depth must be documented for each instrument so that this routine can be performed properly.

A depthPP.txt configuration file allows the user to define which pressure sensor can be taken into account in order to compute the depth of an instrument without any pressure information. See example below :

{{{
% This file specifies how Depth should be computed when an instrument on a 
% mooring doesn't have any pressure sensor. It enables the user to restrict 
% the other instruments in the mooring with pressure information to be
% considered when calculating Depth for another without pressure information. 
% It is used by the depthPP.m function. 
%
% Each line is of the format:
%
%   option,     value
%
% where option can be used as follow :
%
% -same_family : Values can be yes or no. If value is yes, then only data 
% sets with strings in their global attribute 'instrument' having something
% in common will be considered. Ex. : In a mooring equiped with an ADCP, a 
% WQM, Aqualoggers 520PT and Aqualoggers 520T, the depth of the Aqualoggers
% 520T will be computed taking into account Aqualoggers 520PT pressure data
% only.
%
% -include : list of space delimited strings to be compared with data sets 
% global attribute 'instrument'. If it has anything in common, this data 
% set will be considered in the Depth computation.
% Ex. : include,        Aqualoggers
%
% -exclude : list of space delimited strings to be compared with data sets 
% global attribute 'instrument'. If it has anything in common, this data 
% set will not be considered in the Depth computation.
% Ex. : exclude,        ADCP WQM

same_family,   no
include,        
exclude,       ADCP
}}}

== Salinity calculation - salinityPP == 

salinityPP adds a salinity variable to the given data sets, if they contain conductivity, temperature and pressure/depth variables.

This function uses the [http://www.teos-10.org/ Gibbs-SeaWater toolbox (TEOS-10)] to derive salinity data from conductivity, temperature and pressure. It adds the salinity data as a new variable in the data sets. Data sets which do not contain conductivity, temperature and pressure/depth variables or which already contain a salinity variable are left unmodified.

== Correction for magnetic declination - magneticDeclinationPP == 

magneticDeclinationPP computes a theoretical magnetic declination, for any data set relying on a magnetic compass, using NOAA's [http://www.ngdc.noaa.gov/geomag/models.shtml Geomag] v7.0 software and the [http://www.ngdc.noaa.gov/IAGA/vmod/igrf.html IGRF11] model. The actual location of the mooring, the instrument nominal depth (if deeper than 1000m, 1000m depth is considereed instead) and the date in the middle of the time_coverage_start and time_coverage_end of the dataset (it is assumed that the declination variation which occurs during the deployment time period is negligeable compared to the angular resolution of the data) are considered as inputs for Geomag. This computed magnetic declination is documented in a global attribute and a correction from it is applied to any data relying on a magnetic compass when it still refers to magnetic North so that it finally refers to true North.

== vertical bin mapping of ADCP data - binMappingVelocityPP == 

binMappingVelocityPP performs a vertical bin-mapping on any ADCP variable that is a function of DIST_ALONG_BEAM so that it becomes a function of HEIGHT_ABOVE_SENSOR. Tilt information is computed from pitch and roll measurements and used to map bins along the beams towards vertical bins (case when tilt is 0). When more than one bin is mapped to the same bin then the mean is computed over those bins.

== Assess CTD cast surface soak - soakStatusPP == 

soakStatusPP adds distinct diagnostic variables for tempreature, conductivity and dissolved oxygen which will help assessing the quality of the surface soak performed before the CTD was lowered. The depth, elapsed time and conductivity frequency are used to infer when the actual surface soak starts and diagnostic variables are created based on the elapsed time. This pre-processing routine is especially useful in conjunction with the CTDDepthBinPP pre-processing routine and the CTDSurfaceSoakQC test for CTD casts that might have not respected a minimum 3min surface soak (most of the early casts in South Australia for example).

This pre-processing routine is for use in profile mode only with .cnv files that haven't been vertically binned by SeaBird software. Indeed, vertically binned files don't have an elapsed time information anymore.

Diagnostic variables have 4-level values:

 * -1: No Soak Status determined (usually elapsed time missing).
 *  0: Pump Off and/or Minimum Soak Interval has not passed - data from sensor supplied by pump.
 *  1: Pump On and first Minimum Soak Interval has passed - data from sensor are probably OK.
 *  2: Pump On and Optimal Soak Interval has passed - data from sensor assumed to be good.

The following default minimum and optimal soak time values for each parameter can be modified by editing the soakStatusPP.txt:

|| Parameter || Minimum soak time in sec || Optimal soak time in sec ||
|| Temperature || 60 || 70 ||
|| Conductivity || 70 || 90 ||
|| Oxygen || 150 || 180 ||

== CTD cast vertical binning - CTDDepthBinPP == 

CTDDepthBinPP averages the data which lies on vertical bins of size 1m (default value configurable via the CTDDepthBinPP.txt file). Only data with previous surface soak diagnostic variables of values >= 1 and SBE flag 'good' are considered during the averaging process. Diagnostic variables are also binned by considering the worst value in the bin.

== Variable transformation - transformPP == 

transformPP prompts the user to select from a list of transformation functions for the variables in the given data sets.

A transformation function is simply a function which performs some arbitrary transformation on a given data set.

A transformPP.txt configuration file allows the user to define which variable is to be applied which transformation. See example below :

{{{
% Default transformations for different variables. Each entry is of the form:
% 
%   variable_name = transform_name
%
% If a variable is not listed, the default is no transformation.
%

DOXRAW = sbe43OxygenTransform
}}}

The following variable transformation is available in the toolbox:

=== Voltage to concentration oxygen data transformation - sbe43OxygenTransform === 

sbe43OxygenTransform is an implementation of SBE43 oxygen voltage to concentration data transformation, specified in [http://www.seabird.com/application_notes/AN64.htm Seabird Application Note 64]:

In order to derive oxygen, the provided data set must contain TEMP, PRES and PSAL variables.
